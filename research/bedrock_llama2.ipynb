{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS's Bedrock to Create a RAG App Answering User Queries on Pre-Loaded PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import streamlit as st\n",
    "\n",
    "## Using Titan Embeddings Model to Generate Embeddings\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "## Data Ingestion\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Vector Embedding and Vector Store\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## LLM Models\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrock Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock=boto3.client(service_name=\"bedrock-runtime\")\n",
    "bedrock_embeddings=BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion():\n",
    "    loader=PyPDFDirectoryLoader(\"artifacts\")\n",
    "    documents=loader.load()\n",
    "    \n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=10000,\n",
    "                                                 chunk_overlap=1000)\n",
    "    \n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(docs):\n",
    "    vectorstore_faiss=FAISS.from_documents(\n",
    "        docs,\n",
    "        bedrock_embeddings\n",
    "    )\n",
    "    vectorstore_faiss.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Llamma LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_llm():\n",
    "    llm=Bedrock(model_id=\"meta.llama2-70b-chat-v1\",\n",
    "                client=bedrock,\n",
    "                model_kwargs={\"max_gen_len\":512}\n",
    "                )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Claude LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claude_llm():\n",
    "    llm=Bedrock(model_id=\"anthropic.claude-v2:1\",\n",
    "                client=bedrock, \n",
    "                # model_kwargs={\"maxTokens\":512}\n",
    "                )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Human: I'm going to give you a document.\n",
    "Then I'm going to ask you a question about it.\n",
    "I'd like you to first write down exact quotes of parts of the document that would help answer the question, and then I'd like you to answer the question using facts from the quoted content, keeping your answer to around 250 words.\n",
    "Here is the document:\n",
    "<document>\n",
    "{context}\n",
    "</document>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_llm(llm, vectorstore_faiss, query):\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore_faiss.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\":3}\n",
    "        ),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    \n",
    "    answer=qa({\"query\": query})\n",
    "    return answer[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_page_config(\"Chat PDF\")\n",
    "st.header(\"Chat with PDF using AWS Bedrock\")\n",
    "\n",
    "user_question = st.text_input(\"Enter your queries here...\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title(\"Update or Create Vector Store:\")\n",
    "    \n",
    "    if st.button(\"Vectors Update\"):\n",
    "        with st.spinner(\"Processing...\"):\n",
    "            docs = data_ingestion()\n",
    "            get_vector_store(docs)\n",
    "            st.success(\"Done\")\n",
    "\n",
    "# Llama2 output\n",
    "    if st.button(\"Llama2 Output\"):\n",
    "        with st.spinner(\"Processing\"):\n",
    "            faiss_index = FAISS.load_local(\"faiss_index\",\n",
    "                                           bedrock_embeddings,\n",
    "                                           allow_dangerous_deserialization=True\n",
    "                                           )\n",
    "            llm=get_llama2_llm()\n",
    "            \n",
    "            st.write(get_response_llm(llm,\n",
    "                                      faiss_index,\n",
    "                                      user_question))\n",
    "            st.success(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
